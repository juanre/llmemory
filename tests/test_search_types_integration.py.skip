"""Integration tests for different search types with expected results."""

# Skip entire file due to async issues
import pytest
pytest.skip("Requires async initialization for embeddings - skipping due to pytest/asyncpg issues", allow_module_level=True)

import os
import pytest_asyncio
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Tuple
from datetime import datetime
from uuid import uuid4

from llmemory import LLMemory, DocumentType, SearchType
from llmemory.models import SearchResult

@pytest.mark.skip(reason="Requires async initialization for embeddings - skipping due to pytest/asyncpg issues")
@pytest.mark.integration
class TestSearchTypesIntegration:
    """Test different search types (text, vector, hybrid) with real documents."""

    @pytest.fixture
    def memory_with_embeddings(self, memory_library) -> LLMemory:
        """Create memory instance with embedded documents."""
        # Skip if no OpenAI key
        openai_key = os.getenv("OPENAI_API_KEY")
        if not openai_key:
            pytest.skip("OpenAI API key required for vector search tests")

        # Use the memory_library fixture which already handles test database setup
        memory = memory_library

        # Load a subset of documents for faster testing
        test_docs = {
            "quantum_computing": {
                "content": """
                Quantum computing leverages quantum mechanical phenomena like superposition and entanglement.
                Unlike classical bits that are either 0 or 1, qubits can exist in superposition of both states.
                This allows quantum computers to perform certain calculations exponentially faster.
                Shor's algorithm can factor large numbers efficiently, threatening current cryptography.
                Google achieved quantum supremacy in 2019 with their Sycamore processor.
                """,
                "keywords": ["quantum", "qubit", "superposition", "entanglement", "Shor", "cryptography"]
            },
            "artificial_intelligence": {
                "content": """
                Artificial Intelligence simulates human intelligence in machines programmed to think and learn.
                Machine learning uses neural networks inspired by the human brain structure.
                Deep learning with CNNs revolutionized computer vision and image recognition.
                Large Language Models like GPT can understand and generate human-like text.
                AI raises ethical concerns about bias, privacy, and the future of employment.
                """,
                "keywords": ["AI", "machine learning", "neural networks", "deep learning", "GPT", "ethics"]
            },
            "climate_change": {
                "content": """
                Climate change is driven by greenhouse gases trapping heat in Earth's atmosphere.
                Carbon dioxide from burning fossil fuels is the primary driver of global warming.
                Rising temperatures cause melting ice caps, sea level rise, and extreme weather.
                Renewable energy like solar and wind power offers solutions to reduce emissions.
                International cooperation through agreements like Paris Climate Accord is essential.
                """,
                "keywords": ["climate", "greenhouse", "carbon", "warming", "renewable", "Paris"]
            }
        }

        # Process documents with embeddings
        for topic, doc_data in test_docs.items():
            # Use sync manager to process documents
            memory._sync_manager.process_document(
                owner_id="test_workspace",
                id_at_origin="test_user",
                document_name=f"{topic}.txt",
                document_type=DocumentType.TEXT,
                content=doc_data["content"],
                metadata={"topic": topic, "keywords": doc_data["keywords"]},
                generate_embeddings=True  # Generate embeddings
            )

        return memory

    def test_text_search_exact_match(self, memory_with_embeddings):
        """Test text search for exact term matches."""
        memory = memory_with_embeddings

        # Search for exact terms
        test_queries = [
            ("quantum supremacy", "quantum_computing"),
            ("Large Language Models", "artificial_intelligence"),
            ("greenhouse gases", "climate_change")
        ]

        for query, expected_topic in test_queries:
            from llmemory.models import SearchQuery
            search_query = SearchQuery(
                owner_id="test_workspace",
                query_text=query,
                search_type=SearchType.TEXT,
                limit=3
            )

            assert len(results) > 0, f"No results for text search: {query}"

            # Verify exact match appears in top result
            top_result = results[0]
            assert query.lower() in top_result.content.lower(), \
                f"Exact match '{query}' should appear in top result"

            # Check document topic
            doc = await memory._async_manager.db.execute_and_fetch_one(
                "SELECT metadata FROM documents WHERE document_id = %s",
                (top_result.document_id,)
            )
            assert doc["metadata"]["topic"] == expected_topic, \
                f"Expected topic {expected_topic} for query '{query}'"

    @pytest.mark.asyncio
    async def test_vector_search_semantic_similarity(self, memory_with_embeddings):
        """Test vector search for semantic similarity."""
        memory = memory_with_embeddings

        # Semantic queries that don't exactly match text
        semantic_queries = [
            {
                "query": "quantum mechanics strange behavior particles",
                "expected_topic": "quantum_computing",
                "related_terms": ["superposition", "quantum", "phenomena"]
            },
            {
                "query": "computer programs that learn from data",
                "expected_topic": "artificial_intelligence",
                "related_terms": ["machine learning", "neural", "programmed"]
            },
            {
                "query": "planet heating environmental crisis",
                "expected_topic": "climate_change",
                "related_terms": ["warming", "climate", "temperature"]
            }
        ]

        for test in semantic_queries:
            results = await memory.search(
                owner_id="test_workspace",
                query_text=test["query"],
                search_type=SearchType.VECTOR,
                limit=3
            )

            assert len(results) > 0, f"No results for semantic query: {test['query']}"

            # Check that top result is semantically related
            top_result = results[0]
            doc = await memory._async_manager.db.execute_and_fetch_one(
                "SELECT metadata FROM documents WHERE document_id = %s",
                (top_result.document_id,)
            )

            assert doc["metadata"]["topic"] == test["expected_topic"], \
                f"Vector search should find {test['expected_topic']} for '{test['query']}'"

            # Verify related terms appear
            content_lower = top_result.content.lower()
            found_terms = [term for term in test["related_terms"] if term in content_lower]
            assert len(found_terms) > 0, \
                f"Should find related terms in vector search results"

    @pytest.mark.asyncio
    async def test_hybrid_search_best_of_both(self, memory_with_embeddings):
        """Test hybrid search combining text and vector approaches."""
        memory = memory_with_embeddings

        # Queries that benefit from both text matching and semantic understanding
        hybrid_queries = [
            {
                "query": "quantum computer applications in cryptography",
                "expected_topic": "quantum_computing",
                "text_match": "cryptography",
                "semantic_match": "Shor's algorithm"
            },
            {
                "query": "AI neural network ethical implications",
                "expected_topic": "artificial_intelligence",
                "text_match": "neural",
                "semantic_match": "bias"
            },
            {
                "query": "climate renewable energy solutions",
                "expected_topic": "climate_change",
                "text_match": "renewable",
                "semantic_match": "reduce emissions"
            }
        ]

        for test in hybrid_queries:
            # Test with different alpha values
            for alpha in [0.3, 0.5, 0.7]:
                results = await memory.search(
                    owner_id="test_workspace",
                    query_text=test["query"],
                    search_type=SearchType.HYBRID,
                    limit=3,
                    alpha=alpha  # Balance between text (0) and vector (1)
                )

                assert len(results) > 0, \
                    f"No results for hybrid search: {test['query']} (alpha={alpha})"

                # Verify top result contains expected content
                top_result = results[0]
                content_lower = top_result.content.lower()

                # Should contain either text match or semantic match
                has_match = (
                    test["text_match"].lower() in content_lower or
                    test["semantic_match"].lower() in content_lower
                )
                assert has_match, \
                    f"Hybrid search should find relevant content for '{test['query']}'"

    @pytest.mark.asyncio
    async def test_search_type_comparison(self, memory_with_embeddings):
        """Compare results across different search types."""
        memory = memory_with_embeddings

        # Query that works differently across search types
        query = "understanding quantum behavior"

        # Get results from each search type
        text_results = await memory.search(
            owner_id="test_workspace",
            query_text=query,
            search_type=SearchType.TEXT,
            limit=5
        )

        vector_results = await memory.search(
            owner_id="test_workspace",
            query_text=query,
            search_type=SearchType.VECTOR,
            limit=5
        )

        hybrid_results = await memory.search(
            owner_id="test_workspace",
            query_text=query,
            search_type=SearchType.HYBRID,
            limit=5
        )

        # Text search might not find exact matches
        # Vector search should find quantum computing content
        assert len(vector_results) > 0, "Vector search should find semantic matches"

        # Hybrid should generally perform best
        if len(hybrid_results) > 0 and len(text_results) > 0:
            # Hybrid should combine benefits of both
            hybrid_topics = set()
            for result in hybrid_results[:3]:
                doc = await memory._async_manager.db.execute_and_fetch_one(
                    "SELECT metadata FROM documents WHERE document_id = %s",
                    (result.document_id,)
                )
                hybrid_topics.add(doc["metadata"]["topic"])

            assert "quantum_computing" in hybrid_topics, \
                "Hybrid search should find quantum content"

    @pytest.mark.asyncio
    async def test_cross_topic_semantic_search(self, memory_with_embeddings):
        """Test vector search finding connections across topics."""
        memory = memory_with_embeddings

        # Queries that could relate to multiple topics
        cross_topic_queries = [
            {
                "query": "advanced technology solving global problems",
                "possible_topics": ["quantum_computing", "artificial_intelligence", "climate_change"],
                "min_topics": 2
            },
            {
                "query": "future innovations and challenges",
                "possible_topics": ["quantum_computing", "artificial_intelligence", "climate_change"],
                "min_topics": 2
            }
        ]

        for test in cross_topic_queries:
            results = await memory.search(
                owner_id="test_workspace",
                query_text=test["query"],
                search_type=SearchType.VECTOR,
                limit=10
            )

            # Collect unique topics
            found_topics = set()
            for result in results:
                doc = await memory._async_manager.db.execute_and_fetch_one(
                    "SELECT metadata FROM documents WHERE document_id = %s",
                    (result.document_id,)
                )
                found_topics.add(doc["metadata"]["topic"])

            # Vector search should find multiple related topics
            assert len(found_topics) >= test["min_topics"], \
                f"Vector search should find connections across {test['min_topics']} topics"

    @pytest.mark.asyncio
    async def test_query_length_impact(self, memory_with_embeddings):
        """Test how query length affects different search types."""
        memory = memory_with_embeddings

        # Short vs long queries
        query_pairs = [
            {
                "short": "quantum",
                "long": "quantum computing with superposition and entanglement for cryptographic applications",
                "expected_topic": "quantum_computing"
            },
            {
                "short": "AI",
                "long": "artificial intelligence machine learning neural networks deep learning applications",
                "expected_topic": "artificial_intelligence"
            }
        ]

        for pair in query_pairs:
            # Test short query
            short_results = await memory.search(
                owner_id="test_workspace",
                query_text=pair["short"],
                search_type=SearchType.HYBRID,
                limit=3
            )

            # Test long query
            long_results = await memory.search(
                owner_id="test_workspace",
                query_text=pair["long"],
                search_type=SearchType.HYBRID,
                limit=3
            )

            # Both should find relevant content
            assert len(short_results) > 0 and len(long_results) > 0

            # Long queries might be more specific
            if long_results:
                top_doc = await memory._async_manager.db.execute_and_fetch_one(
                    "SELECT metadata FROM documents WHERE document_id = %s",
                    (long_results[0].document_id,)
                )
                assert top_doc["metadata"]["topic"] == pair["expected_topic"], \
                    "Longer, more specific queries should accurately find the topic"

    @pytest.mark.asyncio
    async def test_search_with_typos(self, memory_with_embeddings):
        """Test how different search types handle typos."""
        memory = memory_with_embeddings

        # Queries with typos
        typo_queries = [
            ("quantm computng", "quantum_computing"),    # Missing letters
            ("artifical inteligence", "artificial_intelligence"),  # Common misspellings
            ("climte chang", "climate_change")           # Missing letters
        ]

        for typo_query, expected_topic in typo_queries:
            # Text search might struggle with typos
            text_results = await memory.search(
                owner_id="test_workspace",
                query_text=typo_query,
                search_type=SearchType.TEXT,
                limit=3
            )

            # Vector search should be more forgiving
            vector_results = await memory.search(
                owner_id="test_workspace",
                query_text=typo_query,
                search_type=SearchType.VECTOR,
                limit=3
            )

            # Vector search should still find relevant content despite typos
            if vector_results:
                top_doc = await memory._async_manager.db.execute_and_fetch_one(
                    "SELECT metadata FROM documents WHERE document_id = %s",
                    (vector_results[0].document_id,)
                )
                # Vector embeddings should handle typos better
                found_topic = top_doc["metadata"]["topic"]
                # Note: This might not always work perfectly with typos
                print(f"Typo query '{typo_query}' found topic: {found_topic}")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-m", "integration"])
